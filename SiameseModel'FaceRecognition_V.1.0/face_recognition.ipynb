{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tarfile\n",
    "import builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gpu: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Layer, Dense, Conv2D, MaxPooling2D, Flatten, Lambda, Dropout\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import tensorflow.keras.backend as keras\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(f'Your gpu: {gpus}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_images_flag = False\n",
    "train_flag = False  \n",
    "save_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.path.curdir\n",
    "\n",
    "data_dir = os.path.join(project_path, 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "dataset_dir = os.path.join(project_path, 'dataset')\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir = os.path.join(project_path,'checkpoints')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "save_model_dir = os.path.join(project_path, 'save_model')\n",
    "os.makedirs(save_model_dir, exist_ok=True)\n",
    "\n",
    "db_dir = os.path.join(project_path, 'db')\n",
    "os.makedirs(project_path, exist_ok=True)\n",
    "\n",
    "lfw_dir = os.path.join(dataset_dir, 'lfw')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(dataset_dir,'lfw.tgz')):\n",
    "    !wget http://vis-www.cs.umass.edu/lfw/lfw.tgz -P {dataset_dir}\n",
    "else:\n",
    "    print('The file lfw.tgz is already downloaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking and Extracting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File .\\dataset\\lfw.tgz not found.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si el directorio \"data\" ya contiene archivos\n",
    "if os.path.exists(lfw_dir) and len(os.listdir(lfw_dir)) > 0:\n",
    "    print(f'The \"lfw_dir\" directory already contains {len(os.listdir(lfw_dir))} files:')\n",
    "else:\n",
    "    # Si el directorio \"lfw_dir\" está vacío, descomprimir el archivo lfw.tgz en una carpeta \"lwf\"\n",
    "    lfw_tgz_path = os.path.join(dataset_dir, 'lfw.tgz')\n",
    "    \n",
    "    if os.path.exists(lfw_tgz_path):\n",
    "    # Descomprimir el archivo lfw.tgz en el directorio lwf dentro de \"dataset\"\n",
    "        with tarfile.open(lfw_tgz_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=dataset_dir)\n",
    "            print('File extracted in the \"lwf\" directory inside \"dataset\".')\n",
    "    else:\n",
    "        print(f\"File {lfw_tgz_path} not found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio 'data' no está vacío 0 files:').\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Verificar si el directorio de destino está vacío\n",
    "if not os.listdir(data_dir) and collect_images_flag:\n",
    "    # Iterar sobre las carpetas en el directorio de origen\n",
    "    for folder_name in os.listdir(lfw_dir):\n",
    "        folder_path = os.path.join(lfw_dir, folder_name)\n",
    "        \n",
    "        # Verificar si es una carpeta\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Contar las imágenes dentro de la carpeta\n",
    "            num_images = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "            \n",
    "            # Si la carpeta tiene más de 5 imágenes, copiarla al directorio de destino\n",
    "            if num_images > 5:\n",
    "                dest_folder_path = os.path.join(data_dir, folder_name)\n",
    "                shutil.copytree(folder_path, dest_folder_path)\n",
    "                # print(f\"Copiada la carpeta: {folder_name} con {num_images} imágenes.\")\n",
    "                \n",
    "else:\n",
    "    print(f\"El directorio 'data' no está vacío {len(os.listdir(data_dir))} files:').\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# Cargar el clasificador Haar para la detección de rostros\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def collect_images(images_path=None):\n",
    "    cap = cv2.VideoCapture(0)  # Abrir la cámara\n",
    "\n",
    "    # Establecer una resolución alta para la cámara (ajustar según el dispositivo)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)  # Ancho de resolución\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)  # Altura de resolución\n",
    "\n",
    "    count_images = 0  # Contador de imágenes guardadas\n",
    "    saved_images = []  # Lista para almacenar las imágenes guardadas\n",
    "\n",
    "    # Definir padding (margen) alrededor del rostro\n",
    "    padding = 25  # Margen adicional para evitar cortes bruscos en el rostro\n",
    "\n",
    "    # Bucle para capturar imágenes\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()  # Leer cada frame de la cámara\n",
    "        if not ret:\n",
    "            print(\"No se pudo acceder a la cámara.\")\n",
    "            break\n",
    "\n",
    "        # Convertir el frame a escala de grises para mejorar la detección\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detectar rostros en el frame\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)  # Ajustar parámetros para detección\n",
    "\n",
    "        # Mostrar rectángulos alrededor de los rostros detectados en el frame\n",
    "        display_frame = frame.copy()  # Copiar el frame original solo para mostrar\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Añadir padding alrededor del rostro\n",
    "            x1 = max(0, x - padding)\n",
    "            y1 = max(0, y - padding)\n",
    "            x2 = min(frame.shape[1], x + w + padding)\n",
    "            y2 = min(frame.shape[0], y + h + padding)\n",
    "            \n",
    "            # Dibujar un rectángulo con padding alrededor del rostro\n",
    "            cv2.rectangle(display_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "        # Mostrar la imagen con la detección de rostros (solo para visualización)\n",
    "        cv2.imshow('Image Collection - Press \"p\" to capture', display_frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Guardar el rostro si se presiona 'p'\n",
    "        if key == ord('p'):\n",
    "            if len(faces) > 0:  # Verificar si se detectó al menos un rostro\n",
    "                for (x, y, w, h) in faces:\n",
    "                    # Recortar el rostro con padding\n",
    "                    x1 = max(0, x - padding)\n",
    "                    y1 = max(0, y - padding)\n",
    "                    x2 = min(frame.shape[1], x + w + padding)\n",
    "                    y2 = min(frame.shape[0], y + h + padding)\n",
    "\n",
    "                    # Recortar la imagen del rostro\n",
    "                    face_img = frame[y1:y2, x1:x2]\n",
    "                    \n",
    "                    # Guardar la imagen solo si se proporciona una ruta\n",
    "                    if images_path is not None:\n",
    "                        imgname = os.path.join(images_path, f'{uuid.uuid1()}.jpg')  # Generar un nombre único\n",
    "                        cv2.imwrite(imgname, face_img, [cv2.IMWRITE_JPEG_QUALITY, 95])  # Guardar con calidad alta\n",
    "                        count_images += 1\n",
    "                        saved_images.append(face_img)  # Añadir la imagen guardada a la lista\n",
    "                        print(f'Imagen guardada: {imgname}')\n",
    "                    else:\n",
    "                        # Si no se proporciona una ruta, solo añadir la imagen a la lista\n",
    "                        saved_images.append(face_img)  # Añadir la imagen a la lista sin guardarla\n",
    "                        count_images += 1\n",
    "                        print('Imagen capturada (no guardada).')\n",
    "\n",
    "            else:\n",
    "                print(\"No se detectó ningún rostro para guardar.\")\n",
    "\n",
    "        # Salir si se presiona 'q'\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Liberar la cámara y cerrar todas las ventanas\n",
    "    cap.release()  \n",
    "    cv2.destroyAllWindows()  \n",
    "    \n",
    "    # Devolver tanto el contador de imágenes como la lista de imágenes guardadas\n",
    "    return count_images, saved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting images is not enabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def setup_image_collection(collect_images_flag):\n",
    "    # Verificar si el flag está en False, si es así, no hacer nada\n",
    "    if not collect_images_flag:\n",
    "        print(\"Collecting images is not enabled.\")\n",
    "        return  # Terminar la función sin hacer nada\n",
    "\n",
    "    # Function that is triggered when the \"Yes\" or \"No\" button is pressed\n",
    "    def on_decision_button_clicked(button):\n",
    "        clear_output()  # Clear the current output (hide previous buttons)\n",
    "        if button.description == \"Yes\":\n",
    "            # If \"Yes\" is chosen, show the text box for folder name and the \"Accept\" button\n",
    "            display(folder_name_label, folder_name_widget, accept_button)\n",
    "        else:\n",
    "            # If \"No\" is chosen, end the flow\n",
    "            print(\"No folder will be created.\")\n",
    "\n",
    "    # Function that is triggered when the \"Accept\" button is pressed\n",
    "    def on_accept_button_clicked(button):\n",
    "        folder_selected = data_dir  # Here you can select a default path\n",
    "        folder_name = folder_name_widget.value\n",
    "        \n",
    "        if folder_name:\n",
    "            new_folder_path = os.path.join(folder_selected, folder_name)\n",
    "            os.makedirs(new_folder_path, exist_ok=True)\n",
    "            print(f\"Folder created: {new_folder_path}\")\n",
    "            if collect_images_flag:\n",
    "                collect_images(new_folder_path)  # Call your processing function\n",
    "        else:\n",
    "            print(\"No folder name was entered.\")\n",
    "\n",
    "    # Widgets\n",
    "    decision_label = widgets.Label(\"Do you want to create a new folder for the images?\")\n",
    "    yes_button = widgets.Button(description=\"Yes\", button_style='success')\n",
    "    no_button = widgets.Button(description=\"No\", button_style='danger')\n",
    "\n",
    "    folder_name_label = widgets.Label(\"Enter the name of the new folder:\")\n",
    "    folder_name_widget = widgets.Text(placeholder=\"Enter the folder name\")\n",
    "    accept_button = widgets.Button(description=\"Accept\", button_style='info')\n",
    "\n",
    "    # Link buttons to their functions\n",
    "    yes_button.on_click(on_decision_button_clicked)\n",
    "    no_button.on_click(on_decision_button_clicked)\n",
    "    accept_button.on_click(on_accept_button_clicked)\n",
    "\n",
    "    # Display the \"Yes\" and \"No\" buttons since collect_images_flag is True\n",
    "    display(decision_label, yes_button, no_button)\n",
    "\n",
    "# Ejemplo de cómo llamar la función\n",
    "#collect_images_flag = True \n",
    "setup_image_collection(collect_images_flag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img, num_variations=1):\n",
    "    data = []\n",
    "    for i in range(num_variations):\n",
    "        img_aug = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1, 2))\n",
    "        img_aug = tf.image.stateless_random_contrast(img_aug, lower=0.6, upper=1, seed=(1, 3))\n",
    "        img_aug = tf.image.stateless_random_flip_left_right(img_aug, seed=(np.random.randint(100), np.random.randint(100)))\n",
    "        img_aug = tf.image.stateless_random_jpeg_quality(img_aug, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100), np.random.randint(100)))\n",
    "        img_aug = tf.image.stateless_random_saturation(img_aug, lower=0.9, upper=1, seed=(np.random.randint(100), np.random.randint(100)))\n",
    "        data.append(img_aug)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def log_processed_person(log_file, person_name):\n",
    "    try:\n",
    "        # Verificar si el archivo existe antes de abrirlo\n",
    "        if not os.path.exists(log_file):\n",
    "            with open(log_file, 'w') as f:\n",
    "                pass  # Crea el archivo si no existe\n",
    "        \n",
    "        with builtins.open(log_file, 'a') as f:\n",
    "            f.write(person_name + '\\n')\n",
    "    except OSError as e:\n",
    "        print(f\"Error al escribir en el archivo de log: {e}\")\n",
    "\n",
    "def augment_in_directory(directory, num_variations):\n",
    "    if os.path.exists(directory):\n",
    "        for file_name in os.listdir(directory):\n",
    "            img_path = os.path.join(directory, file_name)\n",
    "\n",
    "            # Verifica que sea un archivo de imagen\n",
    "            if os.path.isfile(img_path) and file_name.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                # Lee la imagen\n",
    "                img = cv2.imread(img_path)\n",
    "\n",
    "                if img is None:\n",
    "                    print(f\"Error al cargar la imagen: {img_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Aplica aumentación\n",
    "                augmented_images = data_aug(tf.convert_to_tensor(img), num_variations)\n",
    "\n",
    "                # Guarda las imágenes aumentadas\n",
    "                for image in augmented_images:\n",
    "                    # Convertir tensor a numpy\n",
    "                    image_np = image.numpy()\n",
    "\n",
    "                    # Asegúrate de que la imagen esté en formato uint8\n",
    "                    if image_np.dtype != np.uint8:\n",
    "                        image_np = (image_np * 255).astype(np.uint8)  # Convertir a uint8\n",
    "\n",
    "                    # Genera un nombre único para la imagen nueva\n",
    "                    new_file_name = '{}.jpg'.format(uuid.uuid1())\n",
    "\n",
    "                    # Guarda la imagen en formato jpg\n",
    "                    cv2.imwrite(os.path.join(directory, new_file_name), image_np)\n",
    "\n",
    "def augment_images_in_directories(images_path, log_file='augmentation_log.txt', num_variations=1):\n",
    "    # Leer los nombres de las personas ya procesadas del archivo de log\n",
    "    processed_people = set()\n",
    "    try:\n",
    "        # Crear el archivo de log si no existe\n",
    "        if not os.path.exists(log_file):\n",
    "            with builtins.open(log_file, 'w') as f:\n",
    "                pass  # Crea el archivo si no existe\n",
    "        \n",
    "        with builtins.open(log_file, 'r') as f:\n",
    "            processed_people = {line.strip() for line in f.readlines()}\n",
    "    except OSError as e:\n",
    "        print(f\"Error al leer el archivo de log: {e}\")\n",
    "        # Si no se puede leer, comenzamos con un conjunto vacío\n",
    "\n",
    "    for person_name in os.listdir(images_path):\n",
    "        person_dir = os.path.join(images_path, person_name)\n",
    "\n",
    "        # Verifica si es un directorio y si ya se procesó esta persona\n",
    "        if os.path.isdir(person_dir) and person_name not in processed_people:\n",
    "            print(f\"Aplicando aumentación a: {person_name}\")\n",
    "\n",
    "            # Aumenta las imágenes en el directorio\n",
    "            augment_in_directory(person_dir, num_variations)\n",
    "\n",
    "            # Registrar el nombre de la persona en el archivo de log\n",
    "            log_processed_person(log_file, person_name)\n",
    "\n",
    "        else:\n",
    "            print(f\"Ya se aplicó aumentación a: {person_name}, omitiendo...\")\n",
    "\n",
    "    print(\"Aumentación completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aumentación completada.\n"
     ]
    }
   ],
   "source": [
    "augment_images_in_directories(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Datasets for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "person_folders = [os.path.join(data_dir, folder) for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder))]\n",
    "persont_count = 200#len(person_folders)\n",
    "pair_p_person = 22\n",
    "test_data_index = round(persont_count*.7)\n",
    "\n",
    "# Preprocesar las imágenes\n",
    "def preprocess_image(image_path):\n",
    "    byte_img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(byte_img, channels=3)  # Asegura que la imagen tenga 3 canales (RGB)\n",
    "    img = tf.image.resize(img, (105, 105))\n",
    "    img = img / 255.0  # Normaliza los valores de los píxeles a [0, 1]\n",
    "    return img\n",
    "\n",
    "def process_pairs(pairs):\n",
    "    images_x = []\n",
    "    images_y = []\n",
    "    labels = []\n",
    "    \n",
    "    for (x, y), label in pairs:\n",
    "        images_x.append(x)\n",
    "        images_y.append(y)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return images_x, images_y, labels\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    value_uint8 = tf.cast(value * 255.0, tf.uint8)  # Escalar de float32 a uint8\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value_uint8).numpy()]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def create_example(image1, image2, label):\n",
    "    feature = {\n",
    "        'image1': _bytes_feature(image1),\n",
    "        'image2': _bytes_feature(image2),\n",
    "        'label': _int64_feature(label)\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "\n",
    "def create_or_load_dataset(data_dir, save_path):\n",
    "    \n",
    "\n",
    "    # Definimos los datasets\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "\n",
    "    test_positives = []\n",
    "    test_negatives = []\n",
    "\n",
    "   \n",
    "    # Generar pares positivos y negativos\n",
    "    for i in range(persont_count):\n",
    "        # Listar las imágenes en la carpeta de la persona i\n",
    "        images_i = tf.data.Dataset.list_files(os.path.join(person_folders[i], '*.jpg')).as_numpy_iterator()\n",
    "        images_i = list(images_i)\n",
    "\n",
    "        if(i >= test_data_index):\n",
    "            for j in range(int(pair_p_person/2)):\n",
    "                #print(j , len(images_i))\n",
    "\n",
    "                test_positives.append((images_i[j], images_i[j+1], 1))  # Etiqueta 1 para pares positivos\n",
    "\n",
    "\n",
    "                # Crear pares negativos con otras personas\n",
    "                index_person_k = random.randint(1, len(person_folders)-1)\n",
    "                person_k = tf.data.Dataset.list_files(os.path.join(person_folders[(i+index_person_k)%len(person_folders)], '*.jpg')).as_numpy_iterator()\n",
    "                person_k = list(person_k)\n",
    "                image_k = random.randint(0, len(person_k)-1)\n",
    "                test_negatives.append((images_i[j], person_k[image_k], 0)) # Etiqueta 0 para pares negativos\n",
    "            continue\n",
    "        \n",
    "\n",
    "\n",
    "        for j in range(int(pair_p_person/2)):\n",
    "            # Crear pares positivos\n",
    "            #print(j , len(images_i))\n",
    "            positive_pairs.append((images_i[j], images_i[j+1], 1))  # Etiqueta 1 para pares positivos\n",
    "\n",
    "\n",
    "            # Crear pares negativos con otras personas\n",
    "            index_person_k = random.randint(1, len(person_folders)-1)\n",
    "            person_k = tf.data.Dataset.list_files(os.path.join(person_folders[(i+index_person_k)%len(person_folders)], '*.jpg')).as_numpy_iterator()\n",
    "            person_k = list(person_k)\n",
    "            image_k = random.randint(0, len(person_k)-1)\n",
    "            negative_pairs.append((images_i[j], person_k[image_k], 0)) # Etiqueta 0 para pares negativos\n",
    "\n",
    "\n",
    "\n",
    "    # Preprocesar y almacenar las imágenes como pares\n",
    "    processed_positive_pairs = [((preprocess_image(x), preprocess_image(y)), label) for x, y, label in positive_pairs]\n",
    "    processed_negative_pairs = [((preprocess_image(x), preprocess_image(y)), label) for x, y, label in negative_pairs]\n",
    "\n",
    "    processed_positive_pairs_test = [((preprocess_image(x), preprocess_image(y)), label) for x, y, label in test_positives] \n",
    "    processed_negative_pairs_test = [((preprocess_image(x), preprocess_image(y)), label) for x, y, label in test_negatives]  \n",
    "\n",
    "\n",
    "\n",
    "    dataset = processed_negative_pairs + processed_positive_pairs\n",
    "    random.shuffle(dataset)  # Mezclar el dataset antes de guardarlo\n",
    "    test_set = processed_positive_pairs_test + processed_negative_pairs_test\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    dataset = dataset + test_set\n",
    "\n",
    "\n",
    "    images_x, images_y, labels = process_pairs(dataset)\n",
    "\n",
    "    # Crear dataset de TensorFlow desde slices de imágenes y etiquetas\n",
    "    return tf.data.Dataset.from_tensor_slices(((images_x, images_y), labels))\n",
    "\n",
    "\n",
    "\n",
    "save_path = 'dataset.tfrecord'\n",
    "if collect_images_flag:\n",
    "    dataset = create_or_load_dataset(data_dir,save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la cardinalidad del dataset\n",
    "if  collect_images_flag:\n",
    "    dataset_size = 0 # dataset.cardinality().numpy()\n",
    "    for _ in dataset:\n",
    "        dataset_size += 1\n",
    "    print(dataset_size)\n",
    "\n",
    "    # Verificar si el dataset es finito o infinito\n",
    "    if dataset_size == tf.data.experimental.INFINITE_CARDINALITY:\n",
    "        print(\"El dataset es infinito.\")\n",
    "    elif dataset_size == tf.data.experimental.UNKNOWN_CARDINALITY:\n",
    "        print(\"El tamaño del dataset es desconocido.\")\n",
    "    else:\n",
    "        # Toma el 70% del conjunto de datos para entrenamiento\n",
    "        train_size = round(test_data_index*pair_p_person)\n",
    "        train_data = dataset.take(train_size)\n",
    "\n",
    "        print(f\"Tamaño del conjunto de entrenamiento: {train_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if collect_images_flag:\n",
    "    test_data = dataset.skip(train_size)\n",
    "    test_data = test_data.take(dataset_size - train_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model Definition\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "- **Input(shape=(105, 105, 3))**: Esta es la capa de entrada del modelo. Recibe imágenes de tamaño **105x105** con **3 canales** (RGB).\n",
    "\n",
    "---\n",
    "\n",
    "### Primera Capa Convolucional + MaxPooling\n",
    "\n",
    "- **Conv2D(64, (10, 10), activation='relu')**: Esta es una **capa convolucional** que aplica **64 filtros** a la imagen de entrada, cada uno de tamaño **10x10**. La función de activación utilizada es **ReLU**, que introduce no linealidad en la red.\n",
    "  - **Entrada**: Imagen de tamaño **105x105x3** (alto x ancho x canales).\n",
    "  - **Salida**: Un conjunto de características (**feature maps**) de tamaño reducido pero con más **canales (profundidad)**, lo que permite que la red aprenda características más complejas.\n",
    "\n",
    "- **MaxPooling2D(pool_size=(2, 2), padding='same')**: Esta es una **capa de reducción de dimensión espacial** que actúa sobre las características extraídas por las capas convolucionales anteriores. Su función principal es **reducir el tamaño** de las representaciones espaciales (ancho y alto) para disminuir la cantidad de parámetros, lo que ayuda a evitar el sobreajuste y mejora la eficiencia computacional.\n",
    "  - **Función**: Toma el valor máximo en regiones de **2x2 píxeles** de las características (feature maps) obtenidas de la convolución. Esto permite reducir la resolución espacial mientras se conservan las características más importantes.\n",
    "  - **pool_size=(2, 2)**: La ventana es de **2x2**, reduciendo el tamaño de la imagen en un factor de 2.\n",
    "  - **padding='same'**: Mantiene las dimensiones similares a las de la entrada añadiendo relleno cuando es necesario.\n",
    "  - **Propósito**: Ayuda a la red a enfocarse en **patrones más globales** en las capas más profundas.\n",
    "\n",
    "---\n",
    "\n",
    "### Segunda Capa Convolucional + MaxPooling\n",
    "\n",
    "- **Conv2D(128, (7, 7), activation='relu')**: Aplica **128 filtros** de tamaño **7x7** a las características que vienen de la primera capa de pooling. La activación es nuevamente **ReLU**.\n",
    "  \n",
    "- **MaxPooling2D(pool_size=(2, 2), padding='same')**: Reduce el tamaño espacial aplicando pooling de **2x2**.\n",
    "\n",
    "---\n",
    "\n",
    "### Tercera Capa Convolucional + MaxPooling\n",
    "\n",
    "- **Conv2D(128, (4, 4), activation='relu')**: Aplica **128 filtros** de tamaño **4x4** sobre las características.\n",
    "  \n",
    "- **MaxPooling2D(pool_size=(2, 2), padding='same')**: Reduce nuevamente el tamaño espacial aplicando pooling de **2x2**.\n",
    "\n",
    "---\n",
    "\n",
    "### Cuarta Capa Convolucional + Aplanamiento (Flatten)\n",
    "\n",
    "- **Conv2D(256, (4, 4), activation='relu')**: Aplica **256 filtros** de tamaño **4x4**. En las capas más profundas, se extraen características más complejas como bordes, texturas y patrones.\n",
    "  \n",
    "- **Flatten()**: Aplana el tensor tridimensional (resultado de la convolución) en un **vector unidimensional** para ser procesado por una capa densa completamente conectada.\n",
    "\n",
    "---\n",
    "\n",
    "### Capa Densa Completamente Conectada\n",
    "\n",
    "- **Dense(4096)**: Es una **capa completamente conectada** con **4096 unidades**.\n",
    "  \n",
    "  - Esta capa actúa como la **representación final** de la imagen de entrada, produciendo un **vector de tamaño 4096**, que será el **embedding** que representa la imagen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un modelo de red convolucional para obtener una representación o embedding de una imagen de entrada de tamaño 105x105 con 3 canales de color (RGB)\n",
    "\n",
    "def create_base_network():\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input = Input(shape= (105,105,3))\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Lambda(lambda  x: keras.l2_normalize(x,axis=1))(x)\n",
    "    x = Lambda(lambda  x: keras.l2_normalize(x,axis=1))(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "\n",
    "def make_embedding(): \n",
    "    inp = Input(shape=(105,105,3), name='input_image')\n",
    "    \n",
    "    # First block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu', input_shape = (105, 105, 3))(inp)\n",
    "    m1 = MaxPooling2D()(c1)\n",
    "    \n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D()(c2)\n",
    "    \n",
    "    # Third block \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D()(c3)\n",
    "    \n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    m4  = MaxPooling2D()(c4)\n",
    "    f1 = Flatten()(m4)\n",
    "    d1 = Dense(4096)(f1)\n",
    "    x = Lambda(lambda  x: keras.l2_normalize(x,axis=1))(d1)\n",
    "    \n",
    "    \n",
    "    return Model(inputs=[inp], outputs=x, name='embedding')\n",
    "\n",
    "# Se define una red neuronal convolucional (CNN) para extraer embeddings (representaciones de características) de imágenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distancia L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen las funciones para calcular la distancia L1 (la suma de las diferencias absolutas) entre dos vectores.\n",
    "\n",
    "    \n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = keras.sum(keras.square(x - y), axis=1, keepdims=True)\n",
    "    return keras.sqrt(keras.maximum(sum_square, keras.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tienes el contenido en formato Markdown, estructurado de manera clara y ordenada:\n",
    "\n",
    "```markdown\n",
    "# Definición del modelo siamés\n",
    "\n",
    "## Función `make_siamese_model()`\n",
    "\n",
    "La función `make_siamese_model()` define y devuelve un modelo siamés, que se utiliza para comparar dos imágenes y determinar si son similares.\n",
    "\n",
    "### Explicación del Código\n",
    "\n",
    "1. **Definición de las Entradas**:\n",
    "   ```python\n",
    "   input_sample_image = Input(shape=(105, 105, 3), name='sample_image')\n",
    "   input_validation_image = Input(shape=(105, 105, 3), name='validation_image')\n",
    "   ```\n",
    "   - Se crean tensores de entrada para las imágenes, cada una con un tamaño de **105x105 píxeles** y **3 canales** (RGB).\n",
    "\n",
    "2. **Modelo de Embeddings**:\n",
    "   ```python\n",
    "   embedding_model = make_embedding()\n",
    "   ```\n",
    "   - Se llama a la función `make_embedding()` que define el modelo de **embeddings**, responsable de extraer características importantes de las imágenes.\n",
    "\n",
    "3. **Codificación de la Imagen de Muestra**:\n",
    "   ```python\n",
    "   encoded_s = embedding_model(input_sample_image)\n",
    "   ```\n",
    "   - La imagen de muestra se pasa por el modelo de embeddings, obteniendo un vector de características que representa dicha imagen.\n",
    "\n",
    "4. **Codificación de la Imagen de Validación**:\n",
    "   ```python\n",
    "   encoded_v = embedding_model(input_validation_image)\n",
    "   ```\n",
    "   - La imagen de validación se codifica para obtener su correspondiente vector de características.\n",
    "\n",
    "5. **Cálculo de la Distancia**:\n",
    "   ```python\n",
    "   distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([encoded_s, encoded_v])\n",
    "   ```\n",
    "   - Se calcula la **distancia L1** entre los dos embeddings. Esta medida indica la similitud, donde valores más bajos indican mayor similitud.\n",
    "\n",
    "6. **Definición del Modelo Completo**:\n",
    "   ```python\n",
    "   model = Model(inputs=[input_sample_image, input_validation_image], outputs=distance, name='siamese_network')\n",
    "   ```\n",
    "   - Se crea el modelo completo, especificando las entradas y la salida. Se le asigna el nombre `'siamese_network'`.\n",
    "\n",
    "7. **Retorno del Modelo**:\n",
    "   ```python\n",
    "   return model\n",
    "   ```\n",
    "   - Finalmente, se devuelve el modelo siamés creado.\n",
    "\n",
    "---\n",
    "\n",
    "## Explicación de las Funciones y Activaciones Utilizadas\n",
    "\n",
    "### 1. `Input`\n",
    "- **Descripción**: Crea un tensor de entrada para el modelo.\n",
    "- **Uso**: Se usa para definir las dimensiones de las imágenes de entrada.\n",
    "\n",
    "### 2. `make_embedding()`\n",
    "- **Descripción**: Define un modelo que extrae características relevantes de las imágenes.\n",
    "- **Uso**: Permite obtener representaciones compactas (embeddings) que encapsulan la información más relevante de las imágenes.\n",
    "\n",
    "### 3. `L1Dist`\n",
    "- **Descripción**: Calcula la distancia L1 (también conocida como distancia Manhattan) entre dos vectores.\n",
    "- **Uso**: Mide la similitud entre los embeddings generados para las dos imágenes. Una distancia más baja indica que las imágenes son más similares.\n",
    "\n",
    "### 4. `Dense`\n",
    "- **Descripción**: Crea una capa completamente conectada.\n",
    "- **Uso**: Se utiliza para combinar las salidas de las capas anteriores en una salida única. En este caso, se utiliza para generar la probabilidad de que las imágenes sean de la misma clase.\n",
    "\n",
    "### 5. Funciones de Activación `ReLU`\n",
    "- **Descripción**: La función de activación ReLU (Rectified Linear Unit) devuelve 0 si la entrada es menor que 0; de lo contrario, devuelve la entrada.\n",
    "- **Uso**: Se utiliza en las capas convolucionales para introducir no linealidad y permitir que la red aprenda características complejas.\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen\n",
    "El modelo siamés construido en esta función compara dos imágenes utilizando un modelo de embeddings para extraer características, calcula la distancia entre estas características y finalmente clasifica si las imágenes son similares o no utilizando una capa densa con activación sigmoide. Las activaciones ReLU se utilizan en las capas convolucionales para facilitar el aprendizaje de características no lineales.\n",
    "```\n",
    "\n",
    "Este contenido está diseñado para ser claro y fácil de leer, proporcionando una comprensión completa de la función `make_siamese_model()` y las activaciones utilizadas en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model(): \n",
    "    \n",
    "    input_image = Input(name='input_img', shape=(105,105,3))\n",
    "    \n",
    "    # Validation image in the network \n",
    "    validation_image = Input(name='validation_img', shape=(105,105,3))\n",
    "    embedding = make_embedding() #create_base_network()\n",
    "    # Combine siamese distance components\n",
    "    encoded_s = embedding(input_image)\n",
    "    encoded_v = embedding(validation_image)\n",
    "    \n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([encoded_s, encoded_v])\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=distance, name='SiameseNetwork')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 105, 105, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_img (InputLayer)    [(None, 105, 105, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         10648896    ['input_img[0][0]',              \n",
      "                                                                  'validation_img[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 1)            0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,648,896\n",
      "Trainable params: 10,648,896\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model = make_siamese_model()\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Note :***\n",
    "> ***I could have chosen a simpler approach using Keras' compile function, but I decided to implement the training process manually.\n",
    "This allowed me to understand in detail each step involved in training the model. Although I haven't tested the following code,\n",
    "it should work fine. Here's the streamlined approach:***\n",
    "\n",
    "```python\n",
    "\n",
    "siamese_model = make_siamese_model()\n",
    "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Precision(), Recall()])\n",
    "\n",
    "history = siamese_model.fit(train_data, \n",
    "                            validation_data=test_data,\n",
    "                            epochs=5,\n",
    "                            batch_size=16)\n",
    "\n",
    "plt.plot(history.history['precision'], label='Precision')\n",
    "plt.plot(history.history['recall'], label='Recall')\n",
    "plt.title('Precision and Recall of Siamese Model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metrics')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de Pérdida en Aprendizaje Automático\n",
    "\n",
    "## ¿Qué son las Funciones de Pérdida?\n",
    "\n",
    "Las funciones de pérdida son componentes esenciales en el entrenamiento de redes neuronales. Cuantifican el error entre las predicciones del modelo y las salidas reales, permitiendo al modelo optimizarse para mejorar su rendimiento.\n",
    "\n",
    "### Propósitos de las Funciones de Pérdida\n",
    "\n",
    "1. **Cuantificación del Error**: Mide la diferencia entre las predicciones del modelo y los valores reales.\n",
    "2. **Guía de Optimización**: Ayuda a actualizar los pesos de la red durante el entrenamiento, minimizando el error.\n",
    "3. **Evaluación del Modelo**: Permite comparar el rendimiento de diferentes modelos.\n",
    "\n",
    "---\n",
    "\n",
    "## Funciones de Pérdida Comunes en Redes Siamesas\n",
    "\n",
    "Las redes siamesas están diseñadas para aprender representaciones y medir la similitud entre pares de entradas, por lo que las funciones de pérdida utilizadas en este contexto deben reflejar este objetivo. A continuación se presentan las más comunes:\n",
    "\n",
    "### 1. **Contrastive Loss**\n",
    "\n",
    "- **Uso**: Evalúa la distancia entre dos entradas. Incentiva que las muestras similares estén más cerca en el espacio de embeddings y las disímiles estén más lejos.\n",
    "\n",
    "- **Fórmula**:\n",
    "  \\[\n",
    "  \\text{Loss} = (1 - y) \\cdot \\frac{1}{2} D^2 + y \\cdot \\frac{1}{2} \\max(0, m - D)^2\n",
    "  \\]\n",
    "  donde:\n",
    "  - \\( D \\) es la distancia entre los embeddings,\n",
    "  - \\( y \\) es la etiqueta (1 para similar, 0 para disímil),\n",
    "  - \\( m \\) es el margen.\n",
    "\n",
    "- **Funcionamiento**:\n",
    "  - Si \\( y = 1 \\) (pares similares), la pérdida penaliza si la distancia entre los embeddings es grande.\n",
    "  - Si \\( y = 0 \\) (pares disímiles), la pérdida penaliza si los embeddings están demasiado cerca, dentro del margen \\( m \\).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Triplet Loss**\n",
    "\n",
    "- **Uso**: Compara tripletas de entradas: un ancla (A), una muestra positiva (P) y una negativa (N). Busca que el ancla esté más cerca del positivo que del negativo por un margen dado.\n",
    "\n",
    "- **Fórmula**:\n",
    "  \\[\n",
    "  \\text{Loss} = \\max(0, D(a, p) - D(a, n) + \\alpha)\n",
    "  \\]\n",
    "  donde:\n",
    "  - \\( a \\) es el ancla,\n",
    "  - \\( p \\) es la muestra positiva,\n",
    "  - \\( n \\) es la muestra negativa,\n",
    "  - \\( \\alpha \\) es un margen.\n",
    "\n",
    "- **Funcionamiento**:\n",
    "  - Penaliza si la distancia entre el ancla y la muestra positiva \\( D(a, p) \\) es mayor o igual que la distancia entre el ancla y la muestra negativa \\( D(a, n) \\) más un margen \\( \\alpha \\).\n",
    "  - Asegura que las muestras positivas estén más cerca del ancla que las negativas en el espacio de embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Binary Crossentropy**\n",
    "\n",
    "- **Uso**: Común en tareas de clasificación binaria. Se puede aplicar en redes siamesas para predecir si dos entradas son similares (1) o no (0).\n",
    "\n",
    "- **Fórmula**:\n",
    "  \\[\n",
    "  \\text{Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i)]\n",
    "  \\]\n",
    "  donde:\n",
    "  - \\( y_i \\) es la etiqueta (1 o 0),\n",
    "  - \\( p_i \\) es la probabilidad predicha por el modelo,\n",
    "  - \\( N \\) es el número de ejemplos.\n",
    "\n",
    "- **Funcionamiento**:\n",
    "  - Mide la divergencia entre la probabilidad predicha \\( p_i \\) y la etiqueta real \\( y_i \\).\n",
    "  - Penaliza las predicciones incorrectas: si el modelo asigna una probabilidad baja a una muestra etiquetada como similar (1), la pérdida será alta, y viceversa.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Hinge Loss**\n",
    "\n",
    "- **Uso**: Se utiliza en Máquinas de Vectores de Soporte (SVM), pero también es aplicable a redes siamesas. Busca que la diferencia entre las salidas de pares similares y disímiles esté por encima de un margen.\n",
    "\n",
    "- **Fórmula**:\n",
    "  \\[\n",
    "  \\text{Loss} = \\max(0, m - (y \\cdot D))\n",
    "  \\]\n",
    "  donde:\n",
    "  - \\( y \\) es la etiqueta (1 o -1),\n",
    "  - \\( D \\) es la distancia entre los embeddings,\n",
    "  - \\( m \\) es un margen.\n",
    "\n",
    "- **Funcionamiento**:\n",
    "  - Penaliza si los pares disímiles están demasiado cerca (dentro del margen).\n",
    "  - Similar a **Contrastive Loss**, pero no requiere etiquetas binarias (1 o 0) y es más común en clasificación marginada.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Focal Loss**\n",
    "\n",
    "- **Uso**: Ampliación de la **Binary Crossentropy** para manejar clases desbalanceadas, centrándose en los ejemplos más difíciles de clasificar.\n",
    "\n",
    "- **Fórmula**:\n",
    "  \\[\n",
    "  \\text{Loss} = - \\alpha (1 - p_t)^\\gamma \\log(p_t)\n",
    "  \\]\n",
    "  donde:\n",
    "  - \\( p_t \\) es la probabilidad predicha para la clase correcta,\n",
    "  - \\( \\alpha \\) y \\( \\gamma \\) son hiperparámetros para ajustar el enfoque en ejemplos difíciles.\n",
    "\n",
    "- **Funcionamiento**:\n",
    "  - Otorga mayor peso a ejemplos mal clasificados, reduciendo la pérdida en ejemplos bien clasificados.\n",
    "  - Útil cuando hay desbalance en las clases.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparación de las Funciones de Pérdida\n",
    "\n",
    "| **Función de Pérdida**   | **Ventajas**                               | **Desventajas**                              |\n",
    "|--------------------------|--------------------------------------------|----------------------------------------------|\n",
    "| **Contrastive Loss**      | Favorece la cercanía de muestras similares y la lejanía de las disímiles. | Requiere pares etiquetados.                  |\n",
    "| **Triplet Loss**          | Fomenta un espacio de embeddings mejor estructurado. | Necesita generar tripletas, lo que puede ser complejo. |\n",
    "| **Binary Crossentropy**   | Fácil de implementar, adecuada para clasificación binaria. | No captura bien la relación entre pares.     |\n",
    "| **Hinge Loss**            | Adecuada para problemas marginados.         | Menos intuitiva en redes siamesas.           |\n",
    "| **Focal Loss**            | Excelente para problemas de clases desbalanceadas. | Más compleja de ajustar (hiperparámetros).   |\n",
    "\n",
    "---\n",
    "\n",
    "## Mejor Función de Pérdida para Redes Siamesas que Comparan Rostros\n",
    "\n",
    "Para una red siamesa que clasifica si dos rostros son el mismo, las funciones **Contrastive Loss** y **Triplet Loss** son las más recomendadas:\n",
    "\n",
    "1. **Contrastive Loss** es ideal cuando se tienen pares de datos etiquetados y se busca que el modelo aprenda a diferenciar entre pares similares y disímiles de manera directa.\n",
    "  \n",
    "2. **Triplet Loss** es efectiva cuando se dispone de un gran conjunto de imágenes y es posible generar tripletas. Permite un mejor ajuste del espacio de embeddings, pero es más costosa en términos de preparación de datos.\n",
    "\n",
    "**Conclusión**: Si tienes buenos datos emparejados, **Contrastive Loss** es la mejor opción. Si puedes generar tripletas eficientemente, **Triplet Loss** puede proporcionar mejores resultados, pero con mayor complejidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    \n",
    "    margin = 1\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    sqaure_pred = keras.square(y_pred)\n",
    "    margin_square = keras.square(keras.maximum(margin - y_pred, 0))\n",
    "    return keras.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n",
    "# Initialize Adam optimizer with a learning rate of 0.0001\n",
    "adam_optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "model_checkpoint = tf.train.Checkpoint(opt=adam_optimizer, siamese_model=siamese_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extraer una imagen del dataset\n",
    "if collect_images_flag:\n",
    "    for (image_x, image_y), label in dataset.take(1):  # Tomar un ejemplo del dataset\n",
    "        # image_x es la primera imagen del par, y image_y es la segunda\n",
    "        # Normalmente, las imágenes están en formato tensor, por lo que debemos convertirlas a formato numpy\n",
    "        image_x = image_x.numpy()\n",
    "        image_y = image_y.numpy()\n",
    "        \n",
    "        # Graficar la primera imagen\n",
    "        plt.figure(figsize=(10,5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow((image_x*255).astype(\"uint8\"))  # Convertir los valores a formato correcto\n",
    "        plt.title(f'Imagen X - Etiqueta: {label.numpy()}')\n",
    "\n",
    "        # Graficar la segunda imagen\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow((image_y*255).astype(\"uint8\"))\n",
    "        plt.title(f'Imagen Y - Etiqueta: {label.numpy()}')\n",
    "\n",
    "        plt.show()\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return keras.mean(keras.equal(y_true, keras.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_siamese_model()\n",
    "rms = RMSprop()\n",
    "model.compile(loss=contrastive_loss, optimizer=adam_optimizer, metrics=[accuracy])\n",
    "if(train_flag):\n",
    "    #train_model(train_data,30)\n",
    "\n",
    "    # Suponiendo que tienes train_data y labels como tensores\n",
    "    train_data_batched = train_data.batch(16)\n",
    "    test_data_batched = test_data.batch(16)\n",
    "    # Luego puedes entrenar el modelo directamente con el dataset\n",
    "    history = model.fit(train_data_batched, epochs=50, validation_data=test_data_batched)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if collect_images_flag:\n",
    "    history_dict = history.history\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values = history_dict['val_loss']\n",
    "    epochs = range(1, (len(history.history['val_accuracy']) + 1))\n",
    "    plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY Learning Curves\n",
    "if collect_images_flag:\n",
    "    history_dict = history.history\n",
    "    loss_values = history_dict['accuracy']\n",
    "    val_loss_values = history_dict['val_accuracy']\n",
    "    epochs = range(1, (len(history.history['accuracy']) + 1))\n",
    "    plt.plot(epochs, loss_values, 'bo', label='Training Acc')\n",
    "    plt.plot(epochs, val_loss_values, 'b', label='Validation Acc')\n",
    "    plt.title('Training and validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_siamese_model(save_model_dir, date_str=None):\n",
    "    custom_objects = {\n",
    "        'L1Dist': euclidean_distance, \n",
    "        'Contrastive_loss': contrastive_loss  \n",
    "    }\n",
    "\n",
    "    if date_str:\n",
    "        # Si se proporciona una fecha, intenta cargar el modelo correspondiente\n",
    "        model_filename = f\"siamese_model_{date_str}.h5\"\n",
    "        model_filepath = os.path.join(save_model_dir, model_filename)\n",
    "        if os.path.exists(model_filepath):\n",
    "            print(f\"Cargando el modelo desde: {model_filepath}\")\n",
    "            return tf.keras.models.load_model(model_filepath, custom_objects=custom_objects)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No se encontró el modelo para la fecha proporcionada: {date_str}\")\n",
    "    else:\n",
    "        # Si no se proporciona una fecha, carga el modelo más reciente\n",
    "        model_files = [f for f in os.listdir(save_model_dir) if f.startswith(\"siamese_model_\") and f.endswith(\".h5\")]\n",
    "        if not model_files:\n",
    "            raise FileNotFoundError(\"No se encontraron modelos en el directorio especificado.\")\n",
    "        \n",
    "        # Ordenar los archivos por fecha y hora en el nombre del archivo\n",
    "        model_files.sort(key=lambda x: datetime.datetime.strptime(x.split(\"_\", 2)[2].split(\".\")[0], \"%Y_%m_%d_%H_%M\"), reverse=True)\n",
    "        latest_model_filename = model_files[0]\n",
    "        latest_model_filepath = os.path.join(save_model_dir, latest_model_filename)\n",
    "        print(f\"Cargando el modelo más reciente desde: {latest_model_filepath}\")\n",
    "        return tf.keras.models.load_model(latest_model_filepath) # custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_date = '2024_08_08_17_19'\n",
    "model_filename = f\"siamese_model_{model_date}.h5\"\n",
    "# Guardar el modelo en el subdirectorio\n",
    "\n",
    "if save_flag:\n",
    "    siamese_model.save(os.path.join(save_model_dir,model_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el modelo más reciente desde: .\\save_model\\siamese_model_2024_08_08_17_19.h5\n",
      "Error loading model: Exception encountered when calling layer \"lambda_1\" (type Lambda).\n",
      "\n",
      "unknown opcode\n",
      "\n",
      "Call arguments received by layer \"lambda_1\" (type Lambda):\n",
      "  • inputs=['tf.Tensor(shape=(None, 4096), dtype=float32)', 'tf.Tensor(shape=(None, 4096), dtype=float32)']\n",
      "  • mask=None\n",
      "  • training=None\n"
     ]
    }
   ],
   "source": [
    "# Para cargar un modelo específico por fecha\n",
    "# model = load_siamese_model(save_model_path, \"2024_08_01_12_00\")\n",
    "\n",
    "# Para cargar el modelo más reciente\n",
    "try:\n",
    "    siamese_model = load_siamese_model(save_model_dir)\n",
    "except Exception as e:\n",
    "    print(f'Error loading model: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 105, 105, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_img (InputLayer)    [(None, 105, 105, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         10648896    ['input_img[0][0]',              \n",
      "                                                                  'validation_img[0][0]']         \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 1)            0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,648,896\n",
      "Trainable params: 10,648,896\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View model summary\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/ 70\n",
    "def preprocess_image_form_img(image):\n",
    "    # Si la imagen es un array de NumPy, convertirla a un tensor de TensorFlow\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = tf.convert_to_tensor(image)\n",
    "    \n",
    "    # Verificar si la imagen es un tensor (debe ser de tipo tf.Tensor)\n",
    "    if not tf.is_tensor(image):\n",
    "        raise ValueError(f\"Se esperaba un tensor o array de imagen, pero se recibió: {type(image)}\")\n",
    "    \n",
    "    # Asegurar que la imagen tiene 3 canales (RGB) si no los tiene\n",
    "    if image.shape[-1] != 3:\n",
    "        raise ValueError(\"La imagen debe tener 3 canales (RGB).\")\n",
    "    \n",
    "    # Redimensionar la imagen\n",
    "    img = tf.image.resize(image, (105, 105))\n",
    "    \n",
    "    # Normalizar los valores de los píxeles a [0, 1]\n",
    "    img = img / 255.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def verify(model, input_image, verific_dir, detection_threshold, verification_threshold):\n",
    "    results = []\n",
    "    \n",
    "    # Preprocesar la imagen de entrada para que tenga el formato adecuado para el modelo\n",
    "    input_img = preprocess_image_form_img(input_image)\n",
    "    \n",
    "    # Iterar sobre cada imagen en el directorio de verificación\n",
    "    for image in os.listdir(verific_dir):\n",
    "        \n",
    "        validation_img = preprocess_image(os.path.join(verific_dir, image))\n",
    "        \n",
    "        result = 1-model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        print(result)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Threshold de detección: contar cuántas predicciones son mayores que el umbral de detección\n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    \n",
    "    # Threshold de verificación: calcular la proporción de predicciones positivas sobre el total de muestras positivas\n",
    "    verification = detection / len(os.listdir(os.path.join(verific_dir))) \n",
    "    \n",
    "    # Verificar si la proporción de predicciones positivas es mayor que el umbral de verificación\n",
    "    verified = verification > verification_threshold\n",
    "    \n",
    "    # Devolver los resultados de las predicciones y el resultado de la verificación\n",
    "    return results, verified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def detect_and_verify_face(siamese_model, db_dir, detection_threshold=0.7, verification_threshold=0.7):\n",
    "    count, input_images = collect_images()  # Assuming this function is already defined\n",
    "\n",
    "    # Check if any image was captured\n",
    "    if count > 0:\n",
    "        input_image = input_images[0]  # Take the first captured image\n",
    "        person_found = False  # Flag to check if a match is found\n",
    "        person_results = []\n",
    "        # Iterate over each person in the database directory\n",
    "        for person_folder in os.listdir(db_dir):\n",
    "            person_dir = os.path.join(db_dir, person_folder)\n",
    "\n",
    "            # Ensure it's a directory\n",
    "            if not os.path.isdir(person_dir):\n",
    "                continue\n",
    "\n",
    "            results, person_verified = verify(siamese_model, input_image, person_dir, detection_threshold, verification_threshold)\n",
    "            person_results.append((results, person_verified))\n",
    "                \n",
    "        return max(person_results, key=lambda x: x[0])\n",
    "    else:\n",
    "        print(\"No image was captured.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No image was captured.\n"
     ]
    }
   ],
   "source": [
    "detect_and_verify_face(siamese_model,db_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
